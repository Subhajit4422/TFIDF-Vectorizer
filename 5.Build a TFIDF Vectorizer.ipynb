{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9464I-uxLiw"
   },
   "source": [
    "# Assignment 5 : Build a TFIDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg2ooa4DxLiz"
   },
   "source": [
    "## Task-1 : Build a TFIDF Vectorizer & compare its results with Sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnV82tg1xLi0"
   },
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "bUsYm9wjxLi1"
   },
   "outputs": [],
   "source": [
    "## SkLearn# Collection of string documents\n",
    "\n",
    "corpus = [\n",
    "     'this is the first document',\n",
    "     'this document is the second document',\n",
    "     'and this is the third one',\n",
    "     'is this the first document',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"https://stackoverflow.com/questions/60042735/\n",
    "how-to-build-a-tfidf-vectorizer-given-a-corpus-and-compare-its-results-using-skl\"\"\"\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math as m\n",
    "import operator \n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.\n",
    "TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.transform\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer=TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output=vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIT FUNCTION FOR TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    vocab=set()\n",
    "    #https://www.w3schools.com/python/ref_func_isinstance.asp\n",
    "    if isinstance(dataset,(list)):\n",
    "        for row in dataset:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                vocab.add(word)\n",
    "        unique_words= sorted(list(vocab))\n",
    "        #https://www.geeksforgeeks.org/enumerate-in-python/\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        return vocab\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
     ]
    }
   ],
   "source": [
    "vocab=fit(corpus)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **BOTH THE VOCABULARY ARE SAME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERM FREQUENCY(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 0,\n",
       " 'document': 0.2,\n",
       " 'first': 0.2,\n",
       " 'is': 0.2,\n",
       " 'one': 0,\n",
       " 'second': 0,\n",
       " 'the': 0.2,\n",
       " 'third': 0,\n",
       " 'this': 0.2}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf(corpus,vocab):\n",
    "    tf={}\n",
    "    if isinstance(corpus,(list)):\n",
    "        for idx,row in enumerate(corpus):\n",
    "            #https://www.geeksforgeeks.org/python-counter-objects-elements/\n",
    "            word_freq=dict(Counter(row.split()))\n",
    "            c=sum(word_freq.values())\n",
    "            \"\"\"\"https://www.w3schools.com/python/ref_dictionary_values\n",
    "            .asp#:~:text=The%20values()%20method%20returns,the%20dictionary%2C%20see%20example%20below.\"\"\"\n",
    "            for act in vocab.keys():\n",
    "                if len(act) < 2:\n",
    "                        continue\n",
    "                if act in word_freq:\n",
    "                    tf[act]=word_freq[act]/c\n",
    "                else:\n",
    "                    tf[act]=0  \n",
    "    return(tf)   \n",
    "\n",
    "tf(corpus,vocab)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF: Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'and': 1.916290731874155,\n",
       " 'document': 1.2231435513142097,\n",
       " 'first': 1.5108256237659907,\n",
       " 'is': 1.0,\n",
       " 'one': 1.916290731874155,\n",
       " 'second': 1.916290731874155,\n",
       " 'the': 1.0,\n",
       " 'third': 1.916290731874155,\n",
       " 'this': 1.0}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def idf(corpus,vocab):\n",
    "    idf={}\n",
    "    N=len(corpus)\n",
    "   \n",
    "    for vky in vocab.keys():\n",
    "        n=0\n",
    "        for idx, row in enumerate((corpus)):\n",
    "            if len(vky) < 2:\n",
    "                continue\n",
    "            if vky in row:\n",
    "                n=n+1             \n",
    "        idf[vky]=1+(m.log((1+N)/(1+float(n))))\n",
    "        \n",
    "    return(idf) \n",
    "\n",
    "idf(corpus,vocab)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both idf are same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfv=idf(corpus,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM FUNCTION :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform(corpus,idfs):\n",
    "    rows=[]\n",
    "    colums=[]\n",
    "    values=[]\n",
    "    tfidf={}\n",
    "    if isinstance(corpus, (list,)):\n",
    "        for idx, row in enumerate(tqdm(corpus)):\n",
    "            lis=[]\n",
    "            lis.append(row)\n",
    "            tfval=tf(lis,vocab) \n",
    "            for word,value in idfs.items():\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                tfidf[word]=idfv[word]*tfval[word]\n",
    "                \n",
    "                if tfidf[word]!=0:\n",
    "                    rows.append(idx)\n",
    "                    colums.append(value)\n",
    "                    values.append(tfidf[word])\n",
    "                l=csr_matrix((values, (rows,colums)), shape=(len(corpus),len(vocab)))# creating the sparse matrix\n",
    "                k=normalize(l,norm='l2') # normalize the sparse matrix(l2 norm)\n",
    "        print(k[0])\n",
    "        print()\n",
    "        print(k[0].toarray()) #converting intro dense matrix\n",
    "        print(\"shape of sparse matrix\",k.shape)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00, 322.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.4697913855799205\n",
      "  (0, 2)\t0.580285823684436\n",
      "  (0, 3)\t0.3840852409148149\n",
      "  (0, 6)\t0.3840852409148149\n",
      "  (0, 8)\t0.3840852409148149\n",
      "\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "shape of sparse matrix (4, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#TFIDF VALUES OF FIRST DOCUMENT STORED IN SPARSE MATRIX AND CONVERTED INTO DENSE MATRIX\n",
    "tranform(corpus,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLwmFZfKxLi4"
   },
   "source": [
    "### SkLearn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "Np4dfQOkxLi4"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(corpus)\n",
    "skl_output = vectorizer.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "-7Om8YpYxLi6",
    "outputId": "0a3bd0f5-4424-4400-944f-4482a80bd799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# sklearn feature names, they are sorted in alphabetic order by default.\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "dTKplK96xLi-",
    "outputId": "53722fa2-6756-4aa0-f179-37b578bb6890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.22314355 1.51082562 1.         1.91629073 1.91629073\n",
      " 1.         1.91629073 1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Here we will print the sklearn tfidf vectorizer idf values after applying the fit method\n",
    "# After using the fit function on the corpus the vocab has 9 words in it, and each has its idf value.\n",
    "\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "-CTiWHygxLjA",
    "outputId": "8d5a9cde-2c29-4afe-f7b4-1547e88dba4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of sklearn tfidf vectorizer output after applying transform method.\n",
    "\n",
    "skl_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "bDKEpbA-xLjD",
    "outputId": "87dafd65-5313-443f-8c6e-1b05cc8c2543"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.38408524091481483\n",
      "  (0, 6)\t0.38408524091481483\n",
      "  (0, 3)\t0.38408524091481483\n",
      "  (0, 2)\t0.5802858236844359\n",
      "  (0, 1)\t0.46979138557992045\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# Here the output is a sparse matrix\n",
    "\n",
    "print(skl_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "3QWo34hexLjF",
    "outputId": "cdc04e08-989f-4bdc-dd7f-f1c82a9f90be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "# sklearn tfidf values for first line of the above corpus.\n",
    "# To understand the output better, here we are converting the sparse output matrix to dense matrix and printing it.\n",
    "# Notice that this output is normalized using L2 normalization. sklearn does this by default.\n",
    "\n",
    "print(skl_output[0].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfIwx5LzxLjI"
   },
   "source": [
    "### Your custom implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "HjuCcJwXxLjJ"
   },
   "outputs": [],
   "source": [
    "# Write your code here.\n",
    "# Make sure its well documented and readble with appropriate comments.\n",
    "# Compare your results with the above sklearn tfidf vectorizer\n",
    "# You are not supposed to use any other library apart from the ones given below\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import operator\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMxBmVZExLjK"
   },
   "source": [
    "## Task-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51j_OtqAxLjL"
   },
   "source": [
    "<font face='georgia'>\n",
    "    <h4><strong>2. Implement max features functionality:</strong></h4>\n",
    "\n",
    "<ul>\n",
    "    <li> As a part of this task you have to modify your fit and transform functions so that your vocab will contain only 50 terms with top idf scores.</li>\n",
    "    <br>\n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "NHxPLlwNxLjL",
    "outputId": "9abd8e08-0e24-4975-9a13-4d3636d60323"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in corpus =  746\n"
     ]
    }
   ],
   "source": [
    "# Below is the code to load the cleaned_strings pickle file provided\n",
    "# Here corpus is of list type\n",
    "\n",
    "import pickle\n",
    "with open('cleaned_strings', 'rb') as f:\n",
    "    corpus = pickle.load(f)\n",
    "    \n",
    "# printing the length of the corpus loaded\n",
    "print(\"Number of documents in corpus = \",len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZULfoOIdxLjQ"
   },
   "source": [
    "**Creating vocabulary of unique words from corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "1_DJnnR3xLjR"
   },
   "outputs": [],
   "source": [
    "def fit(dataset):\n",
    "    vocab=set()\n",
    "    #https://www.w3schools.com/python/ref_func_isinstance.asp\n",
    "    if isinstance(dataset,(list)):\n",
    "        for row in dataset:\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                vocab.add(word)\n",
    "        unique_words= sorted(list(vocab))\n",
    "        #https://www.geeksforgeeks.org/enumerate-in-python/\n",
    "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
    "        return vocab\n",
    "        print(\"you need to pass list of sentance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(corpus,vocab):\n",
    "    idf={}\n",
    "    N=len(corpus)\n",
    "    idfsorted={}\n",
    "    idf50={}\n",
    "    vocab50={}\n",
    "    i=0\n",
    "   \n",
    "    for vky in vocab.keys():\n",
    "        n=0\n",
    "        for idx, row in enumerate((corpus)):\n",
    "            if len(vky) < 2:\n",
    "                continue\n",
    "            if vky in row:\n",
    "                n=n+1             \n",
    "        idf[vky]=1+(m.log((1+N)/(1+float(n))))\n",
    "        \n",
    "    for k in sorted(idf,key=idf.get,reverse=True):\n",
    "        idfsorted[k]=idf[k]\n",
    "        z=Counter(idfsorted)\n",
    "        top=z.most_common(50)\n",
    "    for a,b in top:\n",
    "        vocab50[a]=i       #added word in vocab and a number value\n",
    "        idf50[a]=b         #added idf value in idf50\n",
    "        i=i+1              #increase number for each word in vocab\n",
    "    return vocab50,idf50    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab50,idf50=idf(corpus,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 idf value and associated words\n",
      "\n",
      "\n",
      "{'aailiyah': 6.922918004572872, 'abandoned': 6.922918004572872, 'abroad': 6.922918004572872, 'abstruse': 6.922918004572872, 'academy': 6.922918004572872, 'accents': 6.922918004572872, 'accessible': 6.922918004572872, 'acclaimed': 6.922918004572872, 'accolades': 6.922918004572872, 'accurately': 6.922918004572872, 'achille': 6.922918004572872, 'ackerman': 6.922918004572872, 'adams': 6.922918004572872, 'added': 6.922918004572872, 'admins': 6.922918004572872, 'admiration': 6.922918004572872, 'admitted': 6.922918004572872, 'adrift': 6.922918004572872, 'adventure': 6.922918004572872, 'aesthetically': 6.922918004572872, 'affected': 6.922918004572872, 'affleck': 6.922918004572872, 'afternoon': 6.922918004572872, 'agreed': 6.922918004572872, 'aimless': 6.922918004572872, 'aired': 6.922918004572872, 'akasha': 6.922918004572872, 'alert': 6.922918004572872, 'alike': 6.922918004572872, 'allison': 6.922918004572872, 'allowing': 6.922918004572872, 'alongside': 6.922918004572872, 'amateurish': 6.922918004572872, 'amazed': 6.922918004572872, 'amazingly': 6.922918004572872, 'amusing': 6.922918004572872, 'amust': 6.922918004572872, 'anatomist': 6.922918004572872, 'angela': 6.922918004572872, 'angelina': 6.922918004572872, 'angry': 6.922918004572872, 'anguish': 6.922918004572872, 'angus': 6.922918004572872, 'animals': 6.922918004572872, 'animated': 6.922918004572872, 'anita': 6.922918004572872, 'anniversary': 6.922918004572872, 'anthony': 6.922918004572872, 'antithesis': 6.922918004572872, 'anyway': 6.922918004572872}\n"
     ]
    }
   ],
   "source": [
    "print('Top 50 idf value and associated words')\n",
    "print(\"\\n\")\n",
    "print(idf50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 words of top idf values\n",
      "\n",
      "\n",
      "{'aailiyah': 0, 'abandoned': 1, 'abroad': 2, 'abstruse': 3, 'academy': 4, 'accents': 5, 'accessible': 6, 'acclaimed': 7, 'accolades': 8, 'accurately': 9, 'achille': 10, 'ackerman': 11, 'adams': 12, 'added': 13, 'admins': 14, 'admiration': 15, 'admitted': 16, 'adrift': 17, 'adventure': 18, 'aesthetically': 19, 'affected': 20, 'affleck': 21, 'afternoon': 22, 'agreed': 23, 'aimless': 24, 'aired': 25, 'akasha': 26, 'alert': 27, 'alike': 28, 'allison': 29, 'allowing': 30, 'alongside': 31, 'amateurish': 32, 'amazed': 33, 'amazingly': 34, 'amusing': 35, 'amust': 36, 'anatomist': 37, 'angela': 38, 'angelina': 39, 'angry': 40, 'anguish': 41, 'angus': 42, 'animals': 43, 'animated': 44, 'anita': 45, 'anniversary': 46, 'anthony': 47, 'antithesis': 48, 'anyway': 49}\n"
     ]
    }
   ],
   "source": [
    "print('50 words of top idf values')\n",
    "print(\"\\n\")\n",
    "print(vocab50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERM FREQUENCY(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(corpus,vocab):\n",
    "    tf={}\n",
    "    if isinstance(corpus,(list)):\n",
    "        for idx,row in enumerate(corpus):\n",
    "            #https://www.geeksforgeeks.org/python-counter-objects-elements/\n",
    "            word_freq=dict(Counter(row.split()))\n",
    "            c=sum(word_freq.values())\n",
    "            \"\"\"\"https://www.w3schools.com/python/ref_dictionary_values\n",
    "            .asp#:~:text=The%20values()%20method%20returns,the%20dictionary%2C%20see%20example%20below.\"\"\"\n",
    "            for act in vocab.keys():\n",
    "                if len(act) < 2:\n",
    "                        continue\n",
    "                if act in word_freq:\n",
    "                    tf[act]=word_freq[act]/c\n",
    "                else:\n",
    "                    tf[act]=0  \n",
    "    return(tf)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRANSFORM FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform(corpus,vocab):\n",
    "    rows=[]\n",
    "    colums=[]\n",
    "    values=[]\n",
    "    tfidf={}\n",
    "    if isinstance(corpus, (list,)):\n",
    "        for idx, row in enumerate(tqdm(corpus)):\n",
    "            lis=[]\n",
    "            lis.append(row) # for each document we are calling the function tf to calculate tf values for a particular document\n",
    "            \n",
    "            tfval=tf(lis,vocab50) \n",
    "            for word,value in idf50.items():\n",
    "                if len(word) < 2:\n",
    "                    continue\n",
    "                tfidf[word]=idf50[word]*tfval[word]\n",
    "                \n",
    "                if tfidf[word]!=0:\n",
    "                    rows.append(idx)\n",
    "                    colums.append(value)\n",
    "                    values.append(tfidf[word])\n",
    "                l=csr_matrix((values, (rows,colums)), shape=(len(corpus),len(vocab50)))# creating the sparse matrix\n",
    "                k=normalize(l,norm='l2',) # normalize the sparse matrix(l2 norm)\n",
    "        print(k[0])\n",
    "        print()\n",
    "        print(k[0].toarray()) #converting intro dense matrix\n",
    "        print(\"shape of sparse matrix\",k.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 746/746 [00:11<00:00, 67.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1.0\n",
      "\n",
      "[[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "shape of sparse matrix (746, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tranform(corpus,vocab) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_3_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
